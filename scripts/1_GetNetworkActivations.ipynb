{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Network Activations for Test Images\n",
    "*Written by Viviane Clay*\n",
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:23:10.568705Z",
     "start_time": "2021-01-08T08:22:50.966099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as c_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:23:10.598618Z",
     "start_time": "2021-01-08T08:23:10.588646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n",
    "Data set and trained models can be downloaded here: http://dx.doi.org/10.17632/zdh4d5ws2z.2 For the paper a subset of the full test set is used. To do this exclude all folders with [x,x,x,1] which are folders containing images labeled as 'puzzle'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:24:52.919538Z",
     "start_time": "2021-01-08T08:24:52.650260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1350 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "balancedTestSetPath = '../../Results/TowerTraining/BalancedTestSet/'\n",
    "datagen = ImageDataGenerator(validation_split=0)\n",
    "test_bal = datagen.flow_from_directory(balancedTestSetPath, class_mode='sparse',\n",
    "                                       batch_size=1350,shuffle=False,subset=\"training\",target_size=(168,168))\n",
    "# change batch_size to 1600 if also testing for puzzles\n",
    "realLabel = []\n",
    "for c,v in test_bal.class_indices.items():\n",
    "    c_ext = np.fromstring(c[1:-1], dtype=int, sep=', ')\n",
    "    realLabel.append(c_ext)\n",
    "\n",
    "def getRealLabel(labelBatch,RL):\n",
    "    newLB = []\n",
    "    for label in labelBatch:\n",
    "        l = RL[int(label)]\n",
    "        newLB.append(l)\n",
    "    return newLB\n",
    "\n",
    "def getConceptSubset(AllConceptExamples, numExp):\n",
    "    subset = np.random.randint(0,AllConceptExamples.shape[0],numExp)\n",
    "    trainExp = AllConceptExamples[subset]\n",
    "    mask = np.ones(AllConceptExamples.shape, bool)\n",
    "    mask[subset] = False\n",
    "    testExp = AllConceptExamples[mask]\n",
    "    return trainExp,  testExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:27:27.831231Z",
     "start_time": "2021-01-08T08:27:16.301564Z"
    }
   },
   "outputs": [],
   "source": [
    "obs,label = test_bal.next()\n",
    "obs = obs/255\n",
    "y = np.array(getRealLabel(label, realLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Network Activations\n",
    "### Embodied Agent\n",
    "Adapt paths to model checkpoints. Checkpoints can be found in the folder 'data/agent_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:33:57.907197Z",
     "start_time": "2021-01-08T08:33:57.503274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /var/folders/g2/tbt9tg416wjbn478nv98kt6c0000gp/T/ipykernel_93923/1277359276.py:44: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /var/folders/g2/tbt9tg416wjbn478nv98kt6c0000gp/T/ipykernel_93923/1277359276.py:39: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../../Results/TowerTraining/models/TowerF4/TowerF4_Cur_NoR7-0/LearningBrain/model-82450000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 15:37:20.959003: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "#external rewards agent (47 days)\n",
    "ckpt_path_ext = '../../Results/TowerTraining/models/TowerF4/TowerF4_Baseline-0/LearningBrain/model-30100120.cptk'\n",
    "#curious & external rewards agent (61 days)\n",
    "ckpt_path_extint = '../../Results/TowerTraining/models/TowerF4/TowerF4_Cur-0/LearningBrain/model-36351394.cptk'\n",
    "#curious agent (129 days)\n",
    "ckpt_path_int = '../../Results/TowerTraining/models/TowerF4/TowerF4_Cur_NoR7-0/LearningBrain/model-82450000.cptk'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def swish(input_activation):\n",
    "    \"\"\"Swish activation function. For more info: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return tf.multiply(input_activation, tf.nn.sigmoid(input_activation))\n",
    "\n",
    "def create_global_steps():\n",
    "    \"\"\"Creates TF ops to track and increment global training step.\"\"\"\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False, dtype=tf.int32)\n",
    "    increment_step = tf.assign(global_step, tf.add(global_step, 1))\n",
    "    return global_step, increment_step\n",
    "\n",
    "\n",
    "global_step, increment_step = create_global_steps()\n",
    "\n",
    "o_size_h = 168\n",
    "o_size_w = 168\n",
    "vec_obs_size = 8\n",
    "num_layers = 2\n",
    "h_size = 256\n",
    "h_size_vec = 256\n",
    "            \n",
    "visual_in = tf.placeholder(shape=[None, o_size_h, o_size_w, 3], dtype=tf.float32,name=\"visual_observation_0\")\n",
    "\n",
    "running_mean = tf.get_variable(\"running_mean\", [vec_obs_size],trainable=False, dtype=tf.float32,initializer=tf.zeros_initializer())\n",
    "running_variance = tf.get_variable(\"running_variance\", [vec_obs_size],trainable=False,dtype=tf.float32,initializer=tf.ones_initializer())\n",
    "\n",
    "def create_vector_observation_encoder(observation_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        hidden_vec = observation_input\n",
    "        for i in range(num_layers):\n",
    "            hidden_vec = tf.layers.dense(hidden_vec, h_size, activation=activation, reuse=reuse,name=\"hidden_{}\".format(i),kernel_initializer=c_layers.variance_scaling_initializer(1.0))\n",
    "    return hidden_vec\n",
    "\n",
    "def create_visual_observation_encoder(image_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],activation=tf.nn.elu, reuse=reuse, name=\"conv_1\")\n",
    "        conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, reuse=reuse, name=\"conv_2\")\n",
    "        hidden_vis = c_layers.flatten(conv2)\n",
    "\n",
    "    with tf.variable_scope(scope + '/' + 'flat_encoding'):\n",
    "        hidden_flat = create_vector_observation_encoder(hidden_vis, h_size, activation,num_layers, scope, reuse)\n",
    "    return hidden_flat\n",
    "\n",
    "\n",
    "visual_encoders = []\n",
    "hidden_state, hidden_visual = None, None\n",
    "\n",
    "encoded_visual = create_visual_observation_encoder(visual_in,h_size,swish,num_layers,\"main_graph_0_encoder0\", False)\n",
    "visual_encoders.append(encoded_visual)\n",
    "hidden_visual = tf.concat(visual_encoders, axis=1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(sess, ckpt_path_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:32:58.041611Z",
     "start_time": "2021-01-08T08:32:56.460808Z"
    }
   },
   "outputs": [],
   "source": [
    "encBal = sess.run(hidden_visual, feed_dict={visual_in: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:34:01.587357Z",
     "start_time": "2021-01-08T08:33:59.965701Z"
    }
   },
   "outputs": [],
   "source": [
    "#change last line of network code to saver.restore(sess, ckpt_path_ext)\n",
    "encExt = sess.run(hidden_visual, feed_dict={visual_in: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:33:55.517587Z",
     "start_time": "2021-01-08T08:33:53.978669Z"
    }
   },
   "outputs": [],
   "source": [
    "#change last line of network code to saver.restore(sess, ckpt_path_extint)\n",
    "encExtInt = sess.run(hidden_visual, feed_dict={visual_in: obs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:31:16.740100Z",
     "start_time": "2021-01-08T08:31:15.914312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vclay/opt/miniconda3/envs/FCMPaper/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"conv_1/Elu:0\", shape=(?, 41, 41, 16), dtype=float32)\n",
      "Tensor(\"conv_2/Elu:0\", shape=(?, 19, 19, 32), dtype=float32)\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 11552), dtype=float32)\n",
      "Tensor(\"dens_1/Mul:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dens_2/Mul:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dens_3/Mul:0\", shape=(?, 12800), dtype=float32)\n",
      "Tensor(\"reshape_2/Reshape:0\", shape=(?, 20, 20, 32), dtype=float32)\n",
      "Tensor(\"deconv_1/Elu:0\", shape=(?, ?, ?, 16), dtype=float32)\n",
      "Tensor(\"deconv_3/Elu:0\", shape=(?, ?, ?, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 15:42:24.535202: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense,MaxPooling2D,TimeDistributed,Input,concatenate,Flatten,Reshape,LSTM,Lambda\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def swish(input_activation):\n",
    "    \"\"\"Swish activation function. For more info: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return tf.multiply(input_activation, tf.nn.sigmoid(input_activation))\n",
    "\n",
    "inImg = Input(batch_shape=(None,168, 168, 3),name=\"input_1\")\n",
    "conv = Conv2D(filters=16, kernel_size=[8, 8], strides=[4, 4],activation=tf.nn.elu, name=\"conv_1\")(inImg)\n",
    "print(conv)\n",
    "conv = Conv2D(filters=32, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, name=\"conv_2\")(conv)\n",
    "print(conv)\n",
    "flat = Reshape((19*19*32,))(conv)\n",
    "print(flat)\n",
    "dens = Dense(256,activation=swish,kernel_initializer=c_layers.variance_scaling_initializer(1.0), name=\"dens_1\")(flat)\n",
    "print(dens)\n",
    "enc = Dense(256,activation=swish,kernel_initializer=c_layers.variance_scaling_initializer(1.0), name=\"dens_2\")(dens)\n",
    "print(enc)\n",
    "de_dens = Dense(20*20*32,activation=swish,kernel_initializer=c_layers.variance_scaling_initializer(1.0), name=\"dens_3\")(enc)\n",
    "print(de_dens)\n",
    "shaped = Reshape((20, 20, 32))(de_dens)\n",
    "print(shaped)\n",
    "de_conv = Conv2DTranspose(filters=16, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, name=\"deconv_1\")(shaped)\n",
    "print(de_conv)\n",
    "\n",
    "prediction = Conv2DTranspose(filters=3, kernel_size=[8, 8], strides=[4, 4],padding='same',activation=tf.nn.elu, name=\"deconv_3\")(de_conv)\n",
    "print(prediction)\n",
    "model = Model(inputs=inImg, outputs=prediction)\n",
    "\n",
    "model.compile(optimizer='adadelta',loss='mean_squared_error',metrics=['accuracy','mse'])\n",
    "# adapt this path. Trained model is in the folder 'data'\n",
    "model.load_weights('../../Results/TowerTraining/Recordings/Standard/3999_16.100/autoencoder/aemodelAdam50E.h5')\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=[model.get_layer('dens_2').output,model.get_layer('deconv_3').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:31:29.128048Z",
     "start_time": "2021-01-08T08:31:24.303448Z"
    }
   },
   "outputs": [],
   "source": [
    "encAE = intermediate_layer_model.predict(obs)\n",
    "encAE = encAE[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:31:42.247032Z",
     "start_time": "2021-01-08T08:31:41.498999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'dense/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_1/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_2/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_3/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_4/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_5/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_6/Softmax:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'dense_7/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "Tensor(\"concat_1:0\", shape=(?, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def swish(input_activation):\n",
    "    \"\"\"Swish activation function. For more info: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return tf.multiply(input_activation, tf.nn.sigmoid(input_activation))\n",
    "\n",
    "o_size_h = 168\n",
    "o_size_w = 168\n",
    "num_layers = 2\n",
    "h_size = 256\n",
    "h_size_vec = 256\n",
    "            \n",
    "visual_in = tf.placeholder(shape=[None, o_size_h, o_size_w, 3], dtype=tf.float32,name=\"visual_observation_0\")\n",
    "labels = tf.placeholder(shape=[None,8], dtype=tf.int64,name=\"labels\")\n",
    "\n",
    "def create_vector_observation_encoder(observation_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        hidden_vec = observation_input\n",
    "        for i in range(num_layers):\n",
    "            hidden_vec = tf.layers.dense(hidden_vec, h_size, activation=activation, reuse=reuse,name=\"hidden_{}\".format(i),kernel_initializer=c_layers.variance_scaling_initializer(1.0))\n",
    "    return hidden_vec\n",
    "\n",
    "def create_visual_observation_encoder(image_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],activation=tf.nn.elu, reuse=reuse, name=\"conv_1\")\n",
    "        conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, reuse=reuse, name=\"conv_2\")\n",
    "        hidden_vis = c_layers.flatten(conv2)\n",
    "\n",
    "    with tf.variable_scope(scope + '/' + 'flat_encoding'):\n",
    "        hidden_flat = create_vector_observation_encoder(hidden_vis, h_size, activation,num_layers, scope, reuse)\n",
    "    return hidden_flat\n",
    "\n",
    "visual_encoders = []\n",
    "\n",
    "encoded_visual = create_visual_observation_encoder(visual_in,h_size,swish,num_layers,\"main_graph_0_encoder0\", False)\n",
    "visual_encoders.append(encoded_visual)\n",
    "hidden = tf.concat(visual_encoders, axis=1)\n",
    "\n",
    "out_acts = []\n",
    "for o in range(8):\n",
    "    out_acts.append(tf.layers.dense(hidden, 2, activation=tf.nn.softmax, use_bias=False,kernel_initializer=c_layers.variance_scaling_initializer(factor=0.01)))\n",
    "print(out_acts)\n",
    "\n",
    "output = tf.concat([tf.multinomial(tf.log(out_acts[k]), 1) for k in range(8)], axis=1)#sample outputs from log probdist\n",
    "print(output)\n",
    "#output = tf.round(out_act)\n",
    "#normalized_logits = tf.identity(normalized_logits_flat, name='action')#has nan in places where prob is negative bc it it log(probs)\n",
    "\n",
    "comparison = tf.equal(labels, output)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(comparison, dtype=tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:32:03.233778Z",
     "start_time": "2021-01-08T08:32:00.569589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Results/TowerTraining/Classifier/Model_flat_weighted/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "# adapt this path. Trained model is in the folder 'data'\n",
    "outRand,actRand = sess.run([output,hidden], feed_dict = {visual_in: obs})\n",
    "saver.restore(sess, \"../../Results/TowerTraining/Classifier/Model_flat_weighted/model.ckpt\")\n",
    "outClass,actClass = sess.run([output,hidden], feed_dict = {visual_in: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:44:43.306926Z",
     "start_time": "2021-01-08T08:44:43.277004Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFlatLabel(label2D):\n",
    "    flatLabels = []\n",
    "    for label in label2D:\n",
    "        flatLabel = np.zeros(8)\n",
    "        if label[0] == 0:\n",
    "            flatLabel[0] = 1\n",
    "        elif label[0] == 1:\n",
    "            flatLabel[1] = 1\n",
    "        elif label[0] == 2:\n",
    "            flatLabel[2] = 1\n",
    "        elif label[0] == 3:\n",
    "            flatLabel[3] = 1\n",
    "        elif label[0] == 4:\n",
    "            flatLabel[4] = 1\n",
    "        if label[1] == 1:\n",
    "            flatLabel[5] = 1\n",
    "        if label[2] == 1:\n",
    "            flatLabel[6] = 1\n",
    "        if label[3] == 1:\n",
    "            flatLabel[7] = 1\n",
    "        flatLabels.append(flatLabel)\n",
    "    return np.array(flatLabels)\n",
    "flatL = getFlatLabel(y)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:44:45.232503Z",
     "start_time": "2021-01-08T08:44:43.790200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 78.6261469387755,\n",
       " 'Accuracies': array([84.10842857, 88.4538    , 84.21      , 72.4986    , 85.027     ,\n",
       "        58.5466    , 77.5386    ]),\n",
       " 'Precisions': array([91.00782645, 98.62420721, 85.52777471, 89.23971613, 96.37859743,\n",
       "        86.2125284 , 96.43816935]),\n",
       " 'Recalls': array([75.71428571, 78.        , 82.4       , 51.2       , 72.8       ,\n",
       "        20.4       , 57.2       ]),\n",
       " 'F1Scores': array([82.65558629, 87.10680125, 83.92670424, 65.06043012, 82.94337239,\n",
       "        32.98348875, 71.80562265])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculte calssifier stats with same amount of positive as negative examples per concept \n",
    "#to do this negative examples are randomly sampled from all possible negative examples\n",
    "\n",
    "accuracies, precisions, recalls, f1scores = [],[],[],[]\n",
    "for c in range(7):\n",
    "    accuracy, precision, recall, f1score = [],[],[],[]\n",
    "    for i in range(1000):#repeat multiple times to avoid effect of random sampling for negative examples\n",
    "        classIDs = np.where(flatL[:,c]==1)[0]\n",
    "        numExp = np.shape(classIDs)[0]\n",
    "        negExpIDs = np.where(flatL[:,c]==0)[0]\n",
    "        np.random.shuffle(negExpIDs)\n",
    "        negExpIDs = negExpIDs[:numExp]\n",
    "\n",
    "        true_pos = np.sum(outClass[classIDs,c] == flatL[classIDs,c])\n",
    "        true_neg = np.sum(outClass[negExpIDs,c] == flatL[negExpIDs,c])\n",
    "        false_pos = np.sum(outClass[negExpIDs,c] != flatL[negExpIDs,c])\n",
    "        false_neg = np.sum(outClass[classIDs,c] != flatL[classIDs,c])\n",
    "\n",
    "        accuracy.append((true_neg+true_pos)/(numExp*2))\n",
    "        precision.append(true_pos/(true_pos+false_pos))\n",
    "        recall.append(true_pos/(true_pos+false_neg))\n",
    "        f1score.append(2*(precision[i]*recall[i]/(precision[i]+recall[i])))\n",
    "    accuracies.append(np.mean(accuracy)*100)\n",
    "    precisions.append(np.mean(precision)*100)\n",
    "    recalls.append(np.mean(recall)*100)\n",
    "    f1scores.append(np.mean(f1score)*100)\n",
    "classifier_stats = {'Accuracy': np.mean(accuracies),\n",
    "            'Accuracies': np.array(accuracies),\n",
    "            'Precisions': np.array(precisions),\n",
    "            'Recalls': np.array(recalls),\n",
    "            'F1Scores': np.array(f1scores)}\n",
    "classifier_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T08:46:06.661415Z",
     "start_time": "2021-01-08T08:46:06.625511Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(balancedTestSetPath + 'encExt.npy',encExt)\n",
    "np.save(balancedTestSetPath + 'encInt.npy',encBal)\n",
    "np.save(balancedTestSetPath + 'encExtInt.npy',encExtInt)\n",
    "np.save(balancedTestSetPath + 'encRand.npy',actRand)\n",
    "np.save(balancedTestSetPath + 'encAE.npy',encAE)\n",
    "np.save(balancedTestSetPath + 'encC.npy',actClass)\n",
    "np.save(balancedTestSetPath + 'outClass.npy',outClass)\n",
    "np.save(balancedTestSetPath + 'labels.npy',y)\n",
    "# adapt this path.\n",
    "figurePath = './Results/TowerTraining/Figures/AgentRewardComparisonsAdaTH/ActivationPatternsNormx2No-01-FlatC-50-50Test/'\n",
    "np.save(figurePath+'classifier_stats.npy',classifier_stats)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
